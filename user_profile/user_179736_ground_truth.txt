Title: What's the best way to find the similarity among these vectors?
Tags: <python><algorithm><computer-science>
Body: <pre><code>v1 = [33, 24, 55, 56]
v2 = [32, 25, 51, 40]
v3 = [ ... ]
v4 = [ ... ]
</code></pre>

<p>Normally, to find which vector is the most similar to v1, I would run v1 against the other vectors with a <strong>cosine similarity algorithm</strong>.</p>

<p>Now, I have a more complex set of vectors with the structure:</p>

<pre><code>v1 = [ { 'a': 4, 'b':9, 'c': 12 ... },
       { 'a', 3, 'g':3, 'b': 33 ... },
       { 'b', 1, 'k': 6, 'n': 19 ... },
       ...
     ]
v2 = [ {}, {}, {} ... ]
v3 = [ {}, {}, {} ... ]
v4 = [ {}, {}, {} ... ]
</code></pre>

<p>Given this structure, how would you calculate similarity? (<em>A good match would be a vector with many keys similar to v1, with values of those keys very similar as v1's values</em>)</p>

<p>btilly's answer:</p>

<pre><code>def cosine_sim_complex(v, w):
    '''
    Complex version of cosine similarity
    '''
    def complicated_dot(v, w):
        dot = 0
        for (v_i, w_i) in zip(v, w):
            #{ _, _ }, {_, _}
            for x in v_i:
                if x in w_i:
                    dot += v_i[x] * w_i[x]
        return float(dot)
    length_v = float(complicated_dot(v, v) ** 0.5)
    length_w = float(complicated_dot(w, w) ** 0.5)
    score = complicated_dot(v, w) /  length_v / length_w
    return score


v1 = [ {'a':44, 'b':21 }, { 'a': 55, 'c': 22 } ]
v2 = [ {'a':99, 'b':21 }, { 'a': 55, 'c': 22 } ]
cosine_sim_complex(v1, v2)
1.01342687531
</code></pre>


Answer: <p>You do the same thing in more dimensions.</p>

<p>Previously you just had 4 dimensions.  Now you have a much larger set of dimensions with 2-dimensional labeling of the indices.  But the math remains the same.  You have a dot product like this untested code:</p>

<pre><code>def complicated_dot(v, w):
    dot = 0
    for (v_i, w_i) in zip(v, w):
        for x in v_i.iterkeys():
            if x in w_i:
                dot += v_i[x] * w_i[x]
    return dot
</code></pre>

<p>And then you can apply the cosine similarity algorithm that you already know.</p>


Title: In Celery, how do I run a task, and then have that task run another task, and keep it going?
Tags: <python><django><data-structures><asynchronous><celery>
Body: <pre><code>#tasks.py
from celery.task import Task
class Randomer(Task):
    def run(self, **kwargs):
        #run Randomer again!!!
        return random.randrange(0,1000000)


&gt;&gt;&gt; from tasks import Randomer
&gt;&gt;&gt; r = Randomer()
&gt;&gt;&gt; r.delay()
</code></pre>

<p>Right now, I run the simple task. And it returns a random number.  But, how do I make it run another task , <strong>inside that task</strong>?</p>


Answer: <p>You can call <code>other_task.delay()</code> from inside <code>Randomer.run</code>; in this case you may want to set <code>Randomer.ignore_result = True</code> (and <code>other_task.ignore_result</code>, and so on).</p>

<p>Remember that celery tasks <code>delay</code> returns instantly, so if you don't put any limit or wait time on the nested calls (or recursive calls), you can reach meltdown pretty quickly.</p>

<p>Instead of recursion or nested tasks, you should consider an infinite loop to avoid stack overflow (no pun intended).</p>

<pre><code>from celery.task import Task
class Randomer(Task):
    def run(self, **kwargs):
        while True:
           do_something(**kwargs)
           time.sleep(600)
</code></pre>


Title: What is the best way to go about modeling something like Digg?
Tags: <python><algorithm><matlab><statistics><scipy>
Body: <p>I'm creating a news platform identical to Digg. How would I model its voting mechanism algorithm?  I'm looking for a logical/mathematical approach.</p>


Answer: <p><a href="https://reddit.com" rel="nofollow noreferrer">https://reddit.com</a> is a better version of the now abandoned Digg and guess what, reddit's code and ranking mechanism is open source.</p>

<p>Find the developer page for reddit <a href="http://code.reddit.com/" rel="nofollow noreferrer">here</a>.</p>

<p><a href="http://amix.dk/blog/post/19588" rel="nofollow noreferrer">How Reddit ranking algorithms work (amix.dk)</a></p>


Title: Using Python's PIL, how do I enhance the contrast/saturation of an image?
Tags: <python><algorithm><image><python-imaging-library>
Body: <p>Just a simple contrast and saturation enhancement.
Nothing fancy.</p>


Answer: <p>Since PIL is dead for the most part. Install the Pillow fork instead, <code>sudo pip install pillow</code>, and use its ImageEnhance module <a href="http://pillow.readthedocs.org/en/3.0.x/reference/ImageEnhance.html" rel="noreferrer">http://pillow.readthedocs.org/en/3.0.x/reference/ImageEnhance.html</a></p>

<pre><code>&gt;&gt;&gt; from PIL import Image, ImageEnhance
&gt;&gt;&gt; image = Image.open('downloads/jcfeb2011.jpg')
&gt;&gt;&gt; contrast = ImageEnhance.Contrast(image)
&gt;&gt;&gt; image.show()
</code></pre>

<p><a href="https://i.sstatic.net/841ZY.jpg" rel="noreferrer"><img src="https://i.sstatic.net/841ZY.jpg" alt="(unenhanced)"></a></p>

<pre><code>&gt;&gt;&gt; contrast.enhance(2).show()
</code></pre>

<p><a href="https://i.sstatic.net/PFDLC.jpg" rel="noreferrer"><img src="https://i.sstatic.net/PFDLC.jpg" alt="(contrast enhanced)"></a></p>


Title: If users are uploading images, and I need to do a lot of resizing/uploading, how should I set up my queue?
Tags: <python><data-structures><amazon-s3><queue><distributed>
Body: <p>When the user POSTS to my server, should I store the picture in the body of the queue, and then have the "worker" servers pull it out and resize/upload to S3?</p>

<p>The reason I'm using a queue is because resizing/uploading 20 images to S3 takes a long time.</p>


Answer: <p>Amazon's <a href="http://aws.amazon.com/sqs/" rel="nofollow">Simple Queue Service</a> may be used to maintain communication between your worker and frontend servers, but keep in mind that it only supports messages up to 64kb, meaning it won't be possible to keep the images in the queue.  This is probably best, filesystems are designed to maintain large files, queue implementations generally aren't.</p>

<p>I would have the user upload directly to S3, then use the SQS to coordinate communication between your worker servers as they process the images and return them to S3.</p>


