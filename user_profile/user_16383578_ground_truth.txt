Title: How to properly implement Minimax AI for Tic Tac Toe?
Tags: <python><algorithm><tic-tac-toe><minimax>
Body: <p>I want to implement Minimax AI for Tic Tac Toe of orders 3, 4, 5.</p>
<p>Rules of the game:</p>
<ul>
<li><p>For an order n Tic Tac Toe game, there is a board comprising of n rows and n columns, for a total of n<sup>2</sup> cells. The board is initially empty.</p>
</li>
<li><p>There are two players, players move alternatively, no player can abstain from moving or move in two consecutive turns. In each turn a player must choose a cell that hasn't been previously chosen.</p>
</li>
<li><p>Games ends when either player has occupied a complete row, column or diagonal of n cells, or all cells are occupied.</p>
</li>
</ul>
<p>There are 3 states a cell can be in, so an naive upper bound of count of states can be calculated using 3<sup>n<sup>2</sup></sup>, disregarding the rules. For order 3 it is 19,683, for 4 43,046,721 and for 5 it is 847,288,609,443.</p>
<p>I have programmatically enumerated all legal states for orders 3 and 4. For order 3, there are 5,478 states reachable if &quot;O&quot; moves first, and 5,478 if &quot;X&quot; moves first, rotations and reflections are counted as distinct boards, for a total of 8,533 unique states reachable. For order 4, 972,2011 states are reachable if either player moves first, for a total of 14,782,023 states.</p>
<p>I don't have the exact number of states for order 5, but based on the fact players move alternatively and ignoring the game over condition, there are 161,995,031,226 states reachable if the game doesn't end when there is a winner. So the number of legal states is less than that, I estimate the error of my calculation is within 10%.</p>
<p>I have previously implemented a working <a href="https://stackoverflow.com/questions/77417037/why-does-utilizing-a-simple-strategy-of-tic-tac-toe-lower-the-ais-win-rate">reinforcement learning AI</a> for Tic Tac Toe order 3, but wasn't satisfied by its performance.</p>
<p>So I have tried to implement Minimax AI for Tic Tac Toe, the only thing relevant I have found is <a href="https://www.geeksforgeeks.org/finding-optimal-move-in-tic-tac-toe-using-minimax-algorithm-in-game-theory" rel="nofollow noreferrer">this</a>, but the code quality is horrible and doesn't actually work.</p>
<p>So I tried to implement my own version based on it.</p>
<p>Because a player has either occupied a cell or not occupied a cell, this is binary, so for one player the state of the board can be represented by n<sup>2</sup> bits, as there are two players we need 2n<sup>2</sup> bits for the for information about the board.</p>
<p>1 indicates the player has occupied a cell, 0 indicates a cell is not occupied by the player. Denote the integers that encode player information as <code>(o, x)</code>, <code>o</code> and <code>x</code> cannot has common set bits, to get the full information of the board, use <code>full = o | x</code>, so a set bit in <code>full</code> means the corresponding cell is occupied by either player, else the cell isn't occupied.</p>
<p>I pack the board into one integer using <code>o &lt;&lt; n * n | x</code> to store the information as efficiently as possible, even with this even storing the information of the encoded states for order 4 takes more than 1GiB RAM. Storing the boards and corresponding legal moves for order 4 takes more than 7GiB RAM (I have 16GiB RAM). The board is unpacked by <code>o = full &gt;&gt; n * n; x = full &amp; (1 &lt;&lt; n * n) - 1</code>.</p>
<p>Counting from left to right, and from top to bottom, cell located at row <code>r</code> column <code>c</code> corresponds to <code>full &amp; 1 &lt;&lt; ((n - r) * n - 1 - c)</code>. A move is set by bit-wise OR <code>|</code>.</p>
<p>A fully occupied board has no unset bits, therefore <code>full.bit_count() == n * n</code>. A board has n rows and n columns and 2 diagonals, winner is determined by generating the bit masks for all 2n+2 lines and iterating through the masks to find if any <code>o &amp; mask == mask</code> or <code>x &amp; mask == mask</code>.</p>
<p>And finally, when there is exactly one gap in a line and all other cells in the same line are occupied by one player, said player can win in the next move. So a rational player should always choose such a gap when it is their turn, to win the game if they occupied the other cells or to prevent the other player from winning. The AI should only consider such gaps when there are such gaps.</p>
<p>Thus, the winning strategy would be to create at least two such gaps simultaneously, thus the opponent can only block one gap allowing the player to win in the next turn, this requires at least 2n - 3 cells to be occupied by the player, assuming the player moves first the other player would have taken 2n - 4 turns, the total number of turns so far would thus be 4n - 7. The next move would be the opponent's, so the AI should seek to win in 4n - 5 turns if it moves first, or 4n - 4 turns if it moves second.</p>
<p>The following is the code I used to enumerate all Tic Tac Toe legal states for order 3 (and order 4, the code for order 4 is omitted for brevity, but it can be obtained by trivially changing some numbers):</p>
<pre><code>from typing import List, Tuple


def pack(line: range, last: int) -&gt; int:
    return (sum(1 &lt;&lt; last - i for i in line), tuple(line))


def generate_lines(n: int) -&gt; List[Tuple[int, Tuple[int]]]:
    square = n * n
    last = square - 1
    lines = []
    for i in range(n):
        lines.extend(
            (
                pack(range(i * n, i * n + n), last),
                pack(range(i, square, n), last),
            )
        )

    lines.extend(
        (
            pack(range(0, square, n + 1), last),
            pack(range((m := n - 1), n * m + 1, m), last),
        )
    )
    return lines


LINES_3 = generate_lines(3)

FULL3 = (1 &lt;&lt; 9) - 1
GAMESTATES_3_P1 = {}
GAMESTATES_3_P2 = {}


def check_state_3(o: int, x: int) -&gt; Tuple[bool, int]:
    for line, _ in LINES_3:
        if o &amp; line == line:
            return True, 0
        elif x &amp; line == line:
            return True, 1

    return (o | x).bit_count() == 9, 2


def process_states_3(board: int, move: bool, states: dict, moves: List[int]) -&gt; None:
    if board not in states:
        o = board &gt;&gt; 9
        x = board &amp; FULL3
        if not check_state_3(o, x)[0]:
            left = 8 + 9 * move
            for i, n in enumerate(moves):
                process_states_3(
                    board | 1 &lt;&lt; left - n, not move, states, moves[:i] + moves[i + 1 :]
                )

        c = len(moves)
        states[board] = {i: 1 &lt;&lt; c for i in moves}


process_states_3(0, 1, GAMESTATES_3_P1, list(range(9)))
process_states_3(0, 0, GAMESTATES_3_P2, list(range(9)))
</code></pre>
<p>The following is my reimplementation of the code found in the linked article.</p>
<pre><code>MINIMAX_STATES_3_P1 = {}
SCORES_3 = (10, -10, 0)

def minimax_search_3(board: int, states: dict, maximize: bool, moves: List[int]) -&gt; int:
    if score := states.get(board):
        return score

    o = board &gt;&gt; 9
    x = board &amp; FULL3
    over, winner = check_state_3(o, x)
    if over:
        score = SCORES_3[winner]
        states[board] = score
        return score

    left = 8 + 9 * maximize
    best, extreme, maximize = (-1e309, max, False) if maximize else (1e309, min, True)
    for i, n in enumerate(moves):
        best = extreme(
            best,
            minimax_search_3(
                board | 1 &lt;&lt; left - n, states, maximize, moves[:i] + moves[i + 1 :]
            ),
        )

    states[board] = best
    return best


minimax_search_3(0, MINIMAX_STATES_3_P1, 1, list(range(9)))
</code></pre>
<p>It doesn't work at all, all scores are either 10, 0 or -10, it doesn't take recursion depth into account. The function found in the article will even repeatedly evaluate the same states over and over again, because the states can be reached in different ways, and the function does redundant calculations, I fixed that by caching.</p>
<p>The AI should at minimum stop recursion when a given number of turns are reached, and wins that occur much later should have less weight, and the score of states should vary based on how many win states they lead. And as mentioned before, when there are gaps the AI should only consider such gaps.</p>
<p>I have tried to fix the problems myself, I wrote the following Minimax-ish function, but I don't actually know Minimax theory and I don't know if it works:</p>
<pre><code>def generate_gaps(lines: List[Tuple[int, Tuple[int]]], l: int):
    k = l * l - 1
    return [
        (sum(1 &lt;&lt; k - n for n in line[:i] + line[i + 1 :]), 1 &lt;&lt; k - line[i], line[i])
        for _, line in lines
        for i in range(l)
    ]


GAPS_3 = generate_gaps(LINES_3, 3)


def find_gaps_3(board: int, player: int) -&gt; int:
    return [i for mask, pos, i in GAPS_3 if player &amp; mask == mask and not board &amp; pos]


MINIMAX_3 = {}


def my_minimax_search_3(
    board: int, states: dict, maximize: bool, moves: List[int], turns: int, depth: int
) -&gt; int:
    if entry := states.get(board):
        return entry[&quot;score&quot;]

    o = board &gt;&gt; 9
    x = board &amp; FULL3
    over, winner = check_state_3(o, x)
    if over:
        score = SCORES_3[winner] * 1 &lt;&lt; depth
        states[board] = {&quot;score&quot;: score}
        return score

    if (full := o | x).bit_count() &gt; turns:
        return 0

    depth -= 1
    left, new = (17, False) if maximize else (8, True)
    gaps = set(find_gaps_3(full, o) + find_gaps_3(full, x))
    weights = {
        n: my_minimax_search_3(
            board | 1 &lt;&lt; left - n, states, new, moves[:i] + moves[i + 1 :], depth
        )
        for i, n in enumerate(moves)
        if not gaps or n in gaps
    }
    score = [-1, 1][maximize] * sum(weights.values())
    states[board] = {&quot;weights&quot;: weights, &quot;score&quot;: score}
    return score


my_minimax_search_3(0, MINIMAX_3, 1, list(range(9)), 9, 9)
</code></pre>
<p>How should I properly implement Minimax for Tic Tac Toe?</p>


Answer: <p>The number of states is not that relevant. Minimax (with depth) will just have to look at the states that are reachable with that depth.</p>
<p>What you need is a good enough evaluation function in case the depth has been reached without a winner. Just returning 0 is not a very informed evaluation function.</p>
<p>A candidate as evaluation function could be based on the number of lines where a player could still win, i.e. those lines where the opponent has no occupation yet. For such lines you could give credit for the number of squares that the player has already occupied in such a line: the more the better. I would make this quadratic, so that just one occupied square counts as 1, but 2 counts as 4, and 3 as 9, ...</p>
<p>With such an evaluation function, it turns out that a depth of 2 is already enough for the AI to never lose a game. I must add that against a bad player it would be much harder to win on a 5x5 board than on a 3x3 board. On a 5x5 board it is very easy to block every possibility for the opponent to win. But all these games are a tie with best play.</p>
<p>Here is an implementation of minimax (implemented with the principle of negamax) with alpha-beta pruning. There is no need to memoize boards, as the depth is shallow.</p>
<pre><code>from random import choice

INF = float(&quot;inf&quot;)

class TicTacToe:    
    def __init__(self, size=3):
        self.players = [0, 0]  # bits are 1 when occupied by player
        self.turn = 0
        self.size = size
        self.wins = []
        self.full = (1 &lt;&lt; (size * size)) - 1
        # set masks for horizontal wins
        mask = (1 &lt;&lt; size) - 1
        for i in range(size):
            self.wins.append(mask)
            mask &lt;&lt;= size
        # set masks for vertical wins
        mask = 0
        for i in range(size):
            mask = (mask &lt;&lt; size) | 1
        for i in range(size):
            self.wins.append(mask)
            mask &lt;&lt;= 1
        # set masks for diagonals
        mask = 0
        for i in range(size):
            mask = (mask &lt;&lt; (size + 1)) | 1
        self.wins.append(mask)
        mask = 0
        for i in range(size):
            mask = (mask &lt;&lt; (size - 1)) | 1
        self.wins.append(mask &lt;&lt; (size - 1))
    
    def move(self, square):
        mask = 1 &lt;&lt; square
        self.players[self.turn] ^= mask
        self.turn = 1 - self.turn

    def undo(self, square):
        mask = 1 &lt;&lt; square
        self.turn = 1 - self.turn
        self.players[self.turn] ^= mask

    def moves(self):
        board = self.players[0] | self.players[1]
        for square in range(self.size ** 2):
            if board &amp; (1 &lt;&lt; square) == 0:
                yield square
    
    def is_won(self):
        x = self.players[1 - self.turn]
        return any(x &amp; mask == mask for mask in self.wins)

    def is_full(self):
        return self.players[0] | self.players[1] == self.full

    def is_over(self):
        return self.is_full() or self.is_won()

    def threats(self, turn):  # for use by heuristic evaluation
        player = self.players[turn]
        opponent = self.players[1 - turn]
        return sum(
            (player &amp; mask).bit_count() ** 2
            for mask in self.wins
            if opponent &amp; mask == 0
        )
    
    def heuristic(self):
        return self.threats(self.turn) - self.threats(1 - self.turn)

    def minimax(self, depth, alpha, beta):
        if self.is_won():
            return None, -INF
        if self.is_full():
            return None, 0
        if depth == 0:
            return None, self.heuristic()
        best_moves = None
        best_score = -INF
        for square in self.moves():
            self.move(square)
            score = -self.minimax(depth - 1, -beta, -alpha)[1]
            self.undo(square)
            if score == best_score:
                best_moves.append(square)  # Collect equally valued moves
            elif score &gt; best_score:
                best_score = score
                best_moves = [square]
                if score &gt; beta:
                    break
            alpha = max(alpha, score)
        return choice(best_moves), best_score

    def best_move(self, depth=2):
        return self.minimax(depth, -INF, INF)[0]

    def random_move(self):  # To allow testing with a stupid opponent
        return choice(list(self.moves()))
        
    def __repr__(self):
        s = &quot;&quot;
        for i in range(self.size * self.size):
            mask = 1 &lt;&lt; i
            s += &quot; &quot; if i % self.size else &quot;\n&quot;
            s += &quot;.XO&quot;[(self.players[0] &amp; mask &gt; 0) + (self.players[1] &amp; mask &gt; 0) * 2]
        return s.strip()


# Play a 4x4 game between AI and random mover 
game = TicTacToe(4)
while not game.is_over():
    move = game.random_move() if game.turn else game.best_move() 
    game.move(move)
    print(game)
    print()
</code></pre>


Title: Efficient way to implement Minimax AI for Tic Tac Toe orders 3, 4, 5?
Tags: <python><algorithm><tic-tac-toe>
Body: <p>I want to create an artificial intelligence that plays Tic Tac Toe better than any human.</p>
<p>I have already written a completely working GUI Tic Tac Toe game with AI players, but all those AI players are bad. So I have created another AI using <a href="https://stackoverflow.com/questions/77417037/why-does-utilizing-a-simple-strategy-of-tic-tac-toe-lower-the-ais-win-rate">reinforcement learning</a>, it works and is guaranteed to not to lose more than 99% of the time, but it doesn't win as often.</p>
<p>I want to do better, so I want to use Minimax algorithm to implement the AI. I have found an <a href="https://www.geeksforgeeks.org/finding-optimal-move-in-tic-tac-toe-using-minimax-algorithm-in-game-theory" rel="nofollow noreferrer">example</a> online, but the code quality is very poor. I am able to reimplement it more efficiently, but I don't know the most efficient way to implement it.</p>
<p>I want the AI for 3x3, 4x4 and 5x5 Tic Tac Toe games. These games end when there is a line (row, column and diagonal) that is completely filled with the same player's pieces, or the whole board is filled.</p>
<p>There are 3 possible states each cell can be in, on order n board there are n<sup>2</sup> cells, so for order n game of Tic Tac Toe there are a total of 3<sup>n<sup>2</sup></sup> possible states, regardless of validity, discounting reflections and rotations. For order 3 there are 19,683 possibilities which is quite small, for order 4 there are 43,046,721 possibilities, that is more than 43 million, a big number, but bruteforceable.
For order 5 that is 847,288,609,443, more than 847 billion, it is a huge number and not bruteforceable.</p>
<p>I have checked all possible order 3 Tic Tac Toe states, and systematically enumerated all possible order 3 Tic Tac Toe states if a given player moves first. There are 5478 states reachable if one player moves first, and a total of 8533 states reachable via normal gameplay.</p>
<p>I want to know, what are more efficient ways to check whether a given state is legal, whether a state is a game over state, and whether a state can let one player win in one move.</p>
<p>A state is legal if it can be reached via normal gameplay, a state is a game over state if there are no empty spots in some line and all pieces on the line are the same, and a state can let one player win in one move if there is exactly one empty spot and all other pieces are the same. When the game is one move away from ending I want the AI only consider such gaps.</p>
<p>A state can be reached via normal gameplay if the absolute difference between the counts of pieces is less than or equal to 1, because players take turns alternatively, one player cannot move in two consecutive turns. It must also satisfy that there are no two winners, because the game ends when there is a winner. And It must also satisfy that the loser's moves are no greater than the winner's, because that would mean the loser made a move after there is a winner, which is illegal.</p>
<p>I have solved the aforementioned problems for 3x3 Tic Tac Toe using loops and sets, implementing the logic I mentioned, but I don't think they are very efficient.</p>
<p>(A board is represented by a flat iterable, empty spots are represented as <code>&quot; &quot;</code>, and players <code>&quot;O&quot;, &quot;X&quot;</code>, the cell with index <code>i</code> corresponds to <code>*divmod(i, 3)</code> on the square board)</p>
<pre><code>LINES = (
    (0, 3, 1),
    (3, 6, 1),
    (6, 9, 1),
    (0, 7, 3),
    (1, 8, 3),
    (2, 9, 3),
    (0, 9, 4),
    (2, 7, 2),
)


def is_valid(board: str) -&gt; bool:
    winners = set()
    winner = None
    for start, stop, step in LINES:
        line = board[start:stop:step]
        if len(set(line)) == 1 and (winner := line[0]) in {&quot;O&quot;, &quot;X&quot;}:
            winners.add(winner)

    return (
        len(winners) &lt;= 1
        and abs(board.count(&quot;O&quot;) - board.count(&quot;X&quot;)) &lt;= 1
        and (not winner or board.count(winner) &gt;= board.count(&quot;OX&quot;.replace(winner, &quot;&quot;)))
    )


def is_playable(board: str) -&gt; bool:
    for start, stop, step in LINES:
        line = board[start:stop:step]
        if len(set(line)) == 1 and line[0] in {&quot;O&quot;, &quot;X&quot;}:
            return False

    return &quot; &quot; in board and abs(board.count(&quot;O&quot;) - board.count(&quot;X&quot;)) &lt;= 1


def check_state(board: str) -&gt; Tuple[bool, str]:
    for start, stop, step in LINES:
        line = board[start:stop:step]
        if len(set(line)) == 1 and (winner := line[0]) in {&quot;O&quot;, &quot;X&quot;}:
            return True, winner

    return &quot; &quot; not in board, None


def find_gaps(board: str, piece: str) -&gt; int:
    gaps = []
    for start, end, step in LINES:
        line = board[start:end:step]
        if line.count(piece) == 2 and &quot; &quot; in line:
            gaps.append(start + line.index(&quot; &quot;) * step)

    return gaps
</code></pre>
<p>What are more efficient ways to achieve the above tasks?</p>
<hr />
<h2>Update</h2>
<p>I did some tests:</p>
<pre><code>In [14]: board = &quot;OXOX     &quot;

In [15]: %timeit board[0:3:1]
109 ns ± 0.806 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)

In [16]: %timeit (board[0], board[1], board[2])
124 ns ± 1.09 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)

In [17]: FILLED = {('X', 'X', 'X'), ('O', 'O', 'O')}

In [18]: %timeit (board[0], board[1], board[2]) in FILLED
162 ns ± 1.39 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)

In [19]: %timeit len(set(board[0:3:1])) == 1
348 ns ± 10.6 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)

In [20]: %timeit tuple(board[0:3:1]) in FILLED
301 ns ± 8.03 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)

In [21]: new_board = [1 if s == 'O' else (-1 if s == 'X' else 0) for s in board]

In [22]: new_board
Out[22]: [1, -1, 1, -1, 0, 0, 0, 0, 0]

In [23]: %timeit sum(new_board[0:3:1])
269 ns ± 9.68 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)

In [24]: %timeit tuple(board[i] for i in (0, 1, 2))
699 ns ± 13 ns per loop (mean ± std. dev. of 7 runs, 1,000,000 loops each)

In [25]: %timeit a, b, c = (0, 1, 2); (board[a], board[b], board[c])
146 ns ± 1.69 ns per loop (mean ± std. dev. of 7 runs, 10,000,000 loops each)
</code></pre>
<p>The most efficient way I can find is to store the indices of cells of rows, columns and diagonals, loop through the collection of indices tuples, and unpack the tuple to use each element to retrieve the cell of the board and build a tuple for that line.</p>
<p>Then whether a line is full can be checked by checking if the tuple is <code>('O',)*n</code> or <code>('X',)*n</code>. Whether there is a gap can be checked by membership checking, there are only n possibilities for a given player where there is a single line that is exactly filled with n-1 pieces of the player and the other spot is empty. So two collections, one for each player, can be used to find such gaps. Using a dictionary results in faster membership checking.</p>
<p>This is the best I can do but I don't know if it is super efficient.</p>


Answer: <p>A simple way to check for a winning position for small boards could be using bit tables. In 3x3 you have 9 squares. Consider the <code>X</code> marks as ones and empty or <code>O</code> marks as zeros... any board can be seen as a number 0...511 (511=2⁹-1).
This means that with an array of 512 booleans checking for if the board contains 3 X marks in a row is just a lookup on <code>wtab[index]</code>.</p>
<p>Thus you could represent the board using two numbers: the positions where <code>X</code> marks are and the positions where <code>O</code> marks are. Even better as a tuple with positions where who is next to play has marks and where the opponent has marks.</p>
<p>For example the function that accepts a position and a valid move and returns the updated position becomes simply:</p>
<pre><code>def play(pos, move):
    # pos[0] is player, pos[1] is opponent
    # we need to add the mark for the player and swap roles
    return (pos[1], pos[0] | (1 &lt;&lt; move))
</code></pre>
<p>and checking if who played just won the game is</p>
<pre><code>if wtab[pos[1]]: ...
</code></pre>
<p>For 4x4 board there are 16 squares (65536 elements, still a reasonable solution... especially if compressing to 1-bit per entry = 8Kb) and for 5x5 there are 25 squares (32M elements, that can be trivially packed to 4Mb... but starts making not much sense IMO, at that size just checking neighbors is probably faster).</p>
<p>Note also that caring about speed and using Python for this kind of problem sounds funny. With Python and this type of programs you're already paying a 100x slowdown compared to C/C++/C#/Java.
Even writing your code with Javascript would give you a 10 times faster program with a similar level of abstraction in terms of language. If using Python at least go for PyPy that for speed is similar to Javascript in my experience.</p>


Title: How to combine overlapping ranges efficiently?
Tags: <python><python-3.x><algorithm><performance>
Body: <p>I have data from multiple csv files and I need to merge the tables into one table. The data in question is text dump of GeoLite2 database, and there are literally millions of rows, simply loading the data into lists takes 2927MiB.</p>
<p>The tables contain information about IP networks, some tables contain information about ASN, some about city, and some others about country, these tables have different keys (IP networks), and they may contain common keys, I intend to merge these tables into one table containing information about ASN, country and city of all networks listed.</p>
<p>The question is related to my previous <a href="https://stackoverflow.com/questions/76693414/how-to-optimize-splitting-overlapping-ranges">question</a>, but it is different.</p>
<p>Imagine an infinite boxes arranged in a line, they are numbered using unique integers, and all are initially empty. This time, all boxes can hold infinitely many values, but they can only hold unique values. Meaning, if you put A into box 0, box 0 contains A, but after that, no matter how many times you put A into box 0, box 0 always contains exactly 1 instance of A. But if you put B into box 0, box 0 now contains A and B. But if you put B into the box again, box 0 still contains 1 instance of A and 1 instance of B.</p>
<p>Now there are many triplets, the first two elements are integers, they correspond to start and end of an integer range (inclusive), each triplet describes a continuous integer range of boxes (meaning the number of every box is the number of the previous box plus one) with the same object.</p>
<p>For example, <code>(0, 10, 'A')</code> means boxes 0 to 10 contain an instance of <code>'A'</code>.</p>
<p>The task is to combine the information from the triplets and describe the state of the boxes in the least amount of triplets, in this case the third elements are <code>set</code>s.</p>
<p>Input <code>(0, 10, 'A')</code> -&gt; Output <code>(0, 10, {'A'})</code>, explanation: boxes 0 to 10 contain an instance of <code>'A'</code>.</p>
<p>Input <code>(0, 10, 'A'), (11, 20, 'A')</code> -&gt; Output <code>(0, 20, {'A'})</code>, explanation: boxes 0 to 10 contain an instance of <code>'A'</code>, and boxes 11 to 20 also contain an instance of <code>'A'</code>, 11 is 10 + 1, so boxes 0 to 20 contain an instance of <code>'A'</code>.</p>
<p>Input <code>(0, 10, 'A'), (20, 30, 'A')</code> -&gt; Output <code>(0, 10, {'A'}), (20, 30, {'A'})</code>, explanation: boxes 0 to 10 contain an instance of <code>'A'</code>, and boxes 20 to 30 also contain an instance of <code>'A'</code>, all other boxes are empty, and 20 is not adjacent to 10, don't merge.</p>
<p>Input <code>(0, 10, 'A'), (11, 20, 'B')</code> -&gt; Output <code>(0, 10, {'A'}), (11, 20, {'B'})</code></p>
<p>Input <code>(0, 10, 'A'), (2, 8, 'B')</code> -&gt; Output <code>(0, 1, {'A'}), (2, 8, {'A', 'B'}), (9, 10, {'A'})</code>, explanation: boxes 0 to 10 have <code>'A'</code>, while boxes 2 to 8 have <code>'B'</code>, so boxes 2 to 8 have <code>{'A', 'B'}</code>.</p>
<p>Input <code>(0, 10, 'A'), (5, 20, 'B')</code> -&gt; Output <code>(0, 4, {'A'}), (5, 10, {'A', 'B'}), (11, 20, {'B'})</code> explanation: same as above.</p>
<p>Input <code>(0, 10, 'A'), (5, 10, 'A')</code> -&gt; Output <code>(0, 10, {'A'})</code>, explanation: boxes 0 to 10 have <code>'A'</code>, the second triplet adds no new information and is garbage, discard it.</p>
<p>My current code that produces correct output for some test cases but raises <code>KeyError</code> for others:</p>
<pre><code>import random
from collections import defaultdict
from typing import Any, List, Tuple

def get_nodes(ranges: List[Tuple[int, int, Any]]) -&gt; List[Tuple[int, int, Any]]:
    nodes = []
    for ini, fin, data in ranges:
        nodes.extend([(ini, False, data), (fin, True, data)])
    return sorted(nodes)


def combine_gen(ranges):
    nodes = get_nodes(ranges)
    stack = set()
    actions = []
    for node, end, data in nodes:
        if not end:
            if (action := (data not in stack)):
                if stack and start &lt; node:
                    yield start, node - 1, stack.copy()
                stack.add(data)
                start = node
            actions.append(action)
        elif actions.pop(-1):
            if start &lt;= node:
                yield start, node, stack.copy()
                start = node + 1
            stack.remove(data)


def merge(segments):
    start, end, data = next(segments)
    for start2, end2, data2 in segments:
        if end + 1 == start2 and data == data2:
            end = end2
        else:
            yield start, end, data
            start, end, data = start2, end2, data2
    yield start, end, data


def combine(ranges):
    return list(merge(combine_gen(ranges)))
</code></pre>
<p>It produces correct output for the following test cases:</p>
<pre><code>sample1 = [(0, 20, 'A'), (10, 40, 'B'), (32, 50, 'C'), (40, 50, 'D'), (45, 50, 'E'), (70, 80, 'F'), (90, 100, 'G'), (95, 120, 'H'), (131, 140, 'I'), (140, 150, 'J')]
sample2 = [(0, 10, 'A'), (0, 1, 'B'), (2, 5, 'C'), (3, 4, 'C'), (6, 7, 'C'), (8, 8, 'D'), (110, 150, 'E'), (250, 300, 'C'), (256, 270, 'D'), (295, 300, 'E'), (500, 600, 'F')]
sample3 = [(0, 100, 'A'), (10, 25, 'B'), (15, 25, 'C'), (20, 25, 'D'), (30, 50, 'E'), (40, 50, 'F'), (60, 80, 'G'), (150, 180, 'H')]
sample4 = [(0, 16, 'red'), (0, 4, 'green'), (2, 9, 'blue'), (2, 7, 'cyan'), (4, 9, 'purple'), (6, 8, 'magenta'), (9, 14, 'yellow'), (11, 13, 'orange'), (18, 21, 'green'), (22, 25, 'green')]
</code></pre>
<p>I won't include the expected output for them here, run my code and you will find out what the outputs are, the outputs are correct.</p>
<p>I have written a function to make test cases and a guaranteed correct but inefficient solution, and my efficient code raises <code>KeyError</code> when fed machine generated inputs.</p>
<pre><code>def make_generic_case(num, lim, dat):
    ranges = []

    for _ in range(num):
        start = random.randrange(lim)
        end = random.randrange(lim)
        if start &gt; end:
            start, end = end, start
        ranges.append([start, end, random.randrange(dat)])

    ranges.sort(key=lambda x: (x[0], -x[1]))
    return ranges


def bruteforce_combine(ranges):
    boxes = defaultdict(set)
    for start, end, data in ranges:
        for n in range(start, end + 1):
            boxes[n].add(data)
    
    boxes = sorted(boxes.items())
    output = []
    lo, cur = boxes.pop(0)
    hi = lo

    for n, data in boxes:
        if cur == data and n - hi == 1:
            hi = n
        else:
            output.append((lo, hi, cur))
            lo = hi = n
            cur = data

    output.append((lo, hi, cur))
    return output
</code></pre>
<p>Because my code <em><strong>isn't working properly I CAN'T post it on Code Review</strong></em>, because Code Review only reviews working code and mine isn't.</p>
<p><em><strong>Answers are required to use <code>make_generic_case(512, 4096, 16)</code> to get test cases and verify proposed solution's correctness against the output of <code>bruteforce_combine</code></strong></em>, <code>bruteforce_combine</code> is by definition correct (my logic is <code>defaultdict(set)</code>).</p>
<p>What is a more efficient way to combine the overlapping ranges?</p>
<hr />
<p>Both existing answers are not ideal, the first gives the correct result but is very inefficient and will never finish processing my millions of rows:</p>
<pre><code>In [5]: for _ in range(256):
   ...:     case = make_generic_case(512, 4096, 16)
   ...:     assert bruteforce_combine(case) == combine(case)

In [6]: case = make_generic_case(512, 4096, 16)

In [7]: %timeit combine(case)
9.3 ms ± 35 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>
<p>The second is much more efficient, but I haven't tested thoroughly yet.</p>
<hr />
<p>I have confirmed the correctness of the code from the second answer, and I have rewritten it to the following:</p>
<pre><code>from collections import Counter

def get_nodes(ranges):
    nodes = []
    for start, end, label in ranges:
        nodes.extend(((start, 0, label), (end + 1, 1, label)))

    return sorted(nodes)


def combine(ranges):
    if not ranges:
        return []
    nodes = get_nodes(ranges)
    labels = set()
    state = Counter()
    result = []
    start = nodes[0][0]
    for node, is_end, label in nodes:
        state[label] += [1, -1][is_end]
        count = state[label]
        if (is_end, count) in {(0, 1), (1, 0)}:
            if start &lt; node:
                if not count or labels:
                    result.append((start, node - 1, labels.copy()))

                start = node

            (labels.remove, labels.add)[count](label)

    return result
</code></pre>
<p>And it is still very inefficient, I need to process literally millions of rows:</p>
<pre><code>In [2]: for _ in range(128):
   ...:     case = make_generic_case(256, 4096, 16)
   ...:     assert bruteforce_combine(case) == combine(case)

In [3]: for _ in range(2048):
   ...:     case = make_generic_case(512, 2048, 16)
   ...:     assert bruteforce_combine(case) == combine(case)

In [4]: case = make_generic_case(2048, 2**64, 32)

In [5]: %timeit combine(case)
4.19 ms ± 112 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)

In [6]: case = make_generic_case(32768, 2**64, 32)

In [7]: %timeit combine(case)
116 ms ± 1.11 ms per loop (mean ± std. dev. of 7 runs, 10 loops each)

In [8]: case = make_generic_case(1048576, 2**64, 32)

In [9]: %timeit combine(case)
5.12 s ± 30.3 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)
</code></pre>
<p>I have data from 6 gigantic CSV files, the total number of rows is:</p>
<pre><code>In [74]: 495209+129884+3748518+1277097+429639+278661
Out[74]: 6359008
</code></pre>
<p>That is well over 6 million, merely loading the data into RAM takes 2.9GiB, and I have only 16GiB RAM. I need a solution that is much more efficient, both in time complexity and space complexity.</p>


Answer: <p>Here is a simple strategy.</p>
<p>You are start with lists of tuples of the form <code>(start, end, label)</code>.</p>
<p>Convert each tuple into 2: <code>(start, 0, label), (end+1, 1, label)</code>.</p>
<p>Sort the tuples. Note that this will put an end on the previous box after the start on the following box, so that we can merge the ranges.</p>
<p>Run through them and produce the answer. Note that we are always emitting ranges that end on the box previous to the one that we are currently processing.</p>
<p>For a large data set, I would actually put the intermediate form into a file, sort that outside of Python, then load from there. But that is an implementation detail. Here is some sample code.</p>
<pre><code>def add_sample (sample, merged=None):
    if merged is None:
        merged = []
    for (start, end, label) in sample:
        merged.append((start, 0, label))
        merged.append((end+1, 1, label))
    return merged

def extract_result (merged):
    merged = sorted(merged)
    state = {}
    if 0 == len(merged):
        return []
    prev_labels = set()
    labels = set()
    state = {}
    answer = []
    start_time = merged[0][0]
    for (time, is_end, label) in merged:
        if is_end == 0:
            new_count = state.get(label, 0) + 1
            if new_count == 1:
                if start_time &lt; time:
                    if 0 &lt; len(labels):
                        answer.append((start_time, time-1, list(sorted(labels))))
                    start_time = time
                labels.add(label)
            state[label] = new_count
        else:
            new_count = state.get(label, 0) - 1
            if new_count == 0:
                if start_time &lt; time:
                    prev_labels = labels.copy()
                    answer.append((start_time, time-1, list(sorted(labels))))
                    start_time = time
                labels.remove(label)
            state[label] = new_count

    return answer

sample1 = [(0, 20, 'A'), (10, 40, 'B'), (32, 50, 'C'), (40, 50, 'D'), (45, 50, 'E'), (70, 80, 'F'), (90, 100, 'G'), (95, 120, 'H'), (131, 140, 'I'), (140, 150, 'J')]
sample2 = [(0, 10, 'A'), (0, 1, 'B'), (2, 5, 'C'), (3, 4, 'C'), (6, 7, 'C'), (8, 8, 'D'), (110, 150, 'E'), (250, 300, 'C'), (256, 270, 'D'), (295, 300, 'E'), (500, 600, 'F')]
sample3 = [(0, 100, 'A'), (10, 25, 'B'), (15, 25, 'C'), (20, 25, 'D'), (30, 50, 'E'), (40, 50, 'F'), (60, 80, 'G'), (150, 180, 'H')]
sample4 = [(0, 16, 'red'), (0, 4, 'green'), (2, 9, 'blue'), (2, 7, 'cyan'), (4, 9, 'purple'), (6, 8, 'magenta'), (9, 14, 'yellow'), (11, 13, 'orange'), (18, 21, 'green'), (22, 25, 'green')]

merged = add_sample(sample1)
merged = add_sample(sample2, merged)
merged = add_sample(sample3, merged)
merged = add_sample(sample4, merged)
print(extract_result(merged))
</code></pre>


Title: How to optimize printing Pascal's Triangle in Python?
Tags: <python><python-3.x><algorithm><optimization><pascals-triangle>
Body: <p>I have implemented the <a href="https://en.wikipedia.org/wiki/Pascal%27s_triangle" rel="nofollow noreferrer">Pascal's triangle</a> in Python, it is pretty efficient, but it isn't efficient enough and there are a few things I don't like.</p>
<p>The Pascal's triangle is like the following:</p>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/23050fcb53d6083d9e42043bebf2863fa9746043" alt="" /></p>
<p>I have read <a href="https://www.geeksforgeeks.org/python-program-to-print-pascals-triangle/" rel="nofollow noreferrer">this useless tutorial</a> and <a href="https://stackoverflow.com/questions/24093387/pascals-triangle-for-python">this question</a>, and the solutions are extremely inefficient, involving factorials and don't use caching.</p>
<p>Instead, I implemented a different algorithm I created myself. My mathematics isn't that good, but I have spotted the following simple recursive relationships:</p>
<p>The triangle starts with a row with only 1 number in it, and that number is 1.</p>
<p>For each subsequent row, the length of the row increment by 1, and the first and last number of the row is 1.</p>
<p>Each number that isn't the first or last, is the sum of the number at the row above it with index equal to the number's index minus 1, and the number at row above it with the same index.</p>
<p>And the rows of the triangle are symmetric.</p>
<p>In other words, if we use zero-based indexing:</p>
<pre class="lang-none prettyprint-override"><code>p(r, 0) = p(r, r) = 1
p(r, c) = p(r - 1, c - 1) + p(r - 1, c)
p(r, c) = p(r, r - c)
</code></pre>
<p>Below is my code:</p>
<pre class="lang-py prettyprint-override"><code>from typing import List

class Pascal_Triangle:
    def __init__(self, rows: int = 0, fill: bool = True):
        self.data = []
        self.length = 0
        if rows:
            self.fill_rows(rows)
        if fill:
            self.fill_values()

    def add_row(self, length: int):
        row = [0] * length
        row[0] = row[-1] = 1
        self.data.append(row)

    def fill_rows(self, rows: int):
        for length in range(self.length + 1, rows + 1):
            self.add_row(length)
        self.length = rows

    def comb(self, a: int, b: int) -&gt; int:
        if not 0 &lt;= b &lt;= a:
            raise ValueError(f'cannot choose {b} elements from a population of {a}')

        if self.length &lt; (length := a + 1):
            self.fill_rows(length)

        return self.at(a, b)

    def at(self, row: int, col: int) -&gt; int:
        if val := self.data[row][row - col]:
            self.data[row][col] = val
            return val

        if val := self.data[row][col]:
            return val

        self.data[row][col] = val = self.at(row - 1, col - 1) + self.at(row - 1, col)
        return val

    def fill_values(self):
        for row in range(2, self.length):
            for col in range(1, row):
                self.at(row, col)

    def get_row(self, row: int) -&gt; List[int]:
        if self.length &lt; (length := row + 1):
            self.fill_rows(length)

        self.fill_values()
        return self.data[row]

    def pretty_print(self):
        print('\n'.join(f&quot;{' ' * (self.length - i)}{' '.join(map(str, row))}&quot; for i, row in enumerate(self.data)))
</code></pre>
<p>First, the output of <code>tri = Pascal_Triangle(12); tri.pretty_print()</code> is extremely ugly:</p>
<pre class="lang-none prettyprint-override"><code>            1
           1 1
          1 2 1
         1 3 3 1
        1 4 6 4 1
       1 5 10 10 5 1
      1 6 15 20 15 6 1
     1 7 21 35 35 21 7 1
    1 8 28 56 70 56 28 8 1
   1 9 36 84 126 126 84 36 9 1
  1 10 45 120 210 252 210 120 45 10 1
 1 11 55 165 330 462 462 330 165 55 11 1
</code></pre>
<p>How can I dynamically adjust the spacing between the elements so that the output looks more like an equilateral triangle?</p>
<p>Second I don't like the recursive function, is there any way that I can get rid of the recursive function and calculate the values using the recursive relationship by iteration, while remembering already computed numbers?</p>
<p>Third, is there a data structure more efficient than my nested lists for the same data? I have thought of <code>numpy.array</code> but arrays need each row to have the same length and arrays can't grow.</p>
<p>Finally can my algorithm be optimized further?</p>
<hr />
<p>The data after calling <code>tri.at(16, 5)</code> is:</p>
<pre class="lang-none prettyprint-override"><code>[[1],
 [1, 1],
 [1, 2, 1],
 [1, 3, 3, 1],
 [1, 4, 6, 4, 1],
 [1, 5, 10, 10, 5, 1],
 [1, 6, 15, 20, 15, 6, 1],
 [1, 7, 21, 35, 35, 21, 0, 1],
 [1, 8, 28, 56, 70, 56, 0, 0, 1],
 [1, 9, 36, 84, 126, 126, 0, 0, 0, 1],
 [1, 10, 45, 120, 210, 252, 0, 0, 0, 0, 1],
 [1, 11, 55, 165, 330, 462, 0, 0, 0, 0, 0, 1],
 [1, 12, 66, 220, 495, 792, 0, 0, 0, 0, 0, 0, 1],
 [1, 0, 78, 286, 715, 1287, 0, 0, 0, 0, 0, 0, 0, 1],
 [1, 0, 0, 364, 1001, 2002, 0, 0, 0, 0, 0, 0, 0, 0, 1],
 [1, 0, 0, 0, 1365, 3003, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1],
 [1, 0, 0, 0, 0, 4368, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1]]
</code></pre>
<hr />
<p>I know I am already doing <a href="https://en.wikipedia.org/wiki/Memoization" rel="nofollow noreferrer">memoization</a>, and that is not what I meant. I want to calculate the unfilled values without ever using a recursive function. Instead of using the recursive definition and going backwards, we can somehow use iteration, start from where the lowest value that was filled and needed for the query, and iterate through all needed numbers, make two copies of each number and go forwards, until the requested index was reached.</p>
<p>The needed numbers can be computed using indexing and mathematics.</p>
<p>In this way there is no recursive function call at all.</p>
<hr />
<h2><em><strong>Update</strong></em></h2>
<p>I have rewrote my code to the following:</p>
<pre><code>class Pascal_Triangle:
    def __init__(self, end_row: int = 2, opt: int = 0):
        self.data = [[1], [1, 1]]
        self.length = 2
        self.opt = [self.add_rows_o0, self.add_rows_o1]
        if end_row &gt; 2:
            self.opt[opt](end_row)

    def add_rows_o0(self, end_row: int):
        last_row = self.data[-1]
        for _ in range(self.length, end_row):
            self.data.append(
                last_row := [1] + [a + b for a, b in zip(last_row, last_row[1:])] + [1]
            )

        self.length = end_row

    def add_rows_o1(self, end_row: int):
        last_row = self.data[-1]
        for n in range(self.length, end_row):
            mid = n // 2 + 1
            row = [0] * (n - 1)
            m = n - 2
            for i, (a, b) in enumerate(zip(last_row, last_row[1:mid])):
                row[i] = row[m - i] = a + b

            self.data.append(last_row := [1] + row + [1])
        self.length = end_row

    def pretty_print(self):
        longest = len(str(self.data[-1][self.length // 2]))
        line_length = (longest + 1) * self.length
        for row in self.data:
            print(&quot; &quot;.join(f&quot;{n:{longest}}&quot; for n in row).center(line_length))
</code></pre>
<p>I have used <a href="https://en.wikipedia.org/wiki/List_comprehension#Python" rel="nofollow noreferrer">list comprehension</a> to generate new rows and got rid of the expensive recursive function call. The code is much faster as a result.</p>
<p>However, I tried to exploit the symmetric nature of the rows and only calculate half of the row and mirror it to get the other half. In this way the number of calculations would be halved.</p>
<p>But it is actually slower:</p>
<pre class="lang-none prettyprint-override"><code>In [257]: %timeit Pascal_Triangle(64, 1)
237 µs ± 7.43 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

In [258]: %timeit Pascal_Triangle(64, 0)
224 µs ± 9.75 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

In [259]: Pascal_Triangle(64, 1).data == Pascal_Triangle(64, 0).data
Out[259]: True
</code></pre>
<p>Why is it slower? And how can I actually skip the unnecessary calculations and make it faster?</p>


Answer: <ol>
<li><p>You can improve the <code>pretty_print</code> by getting the length (as string) of the longest number and using that as the basis for all the numbers' width; also using <code>str.center</code> might be easier.</p>
<pre><code> def pretty_print(self):
     longest = max(len(str(n)) for row in self.data for n in row)
     line_length = (longest + 1) * self.length
     for row in self.data:
         print(' '.join(f'{n:{longest}}' for n in row).center(line_length))
</code></pre>
</li>
<li><p>With this check <code>if val := self.data[row][col]: return val</code>, you are already doing that, and each value is calculated exactly once. You could make it purely iterative in <code>fill_values</code> directly, and drop the <code>at</code> method entirely, though:</p>
<pre><code> def fill_values(self):
     for row in range(2, self.length):
         for col in range(1, row):
             self.data[row][col] = self.data[row - 1][col - 1] + self.data[row - 1][col]
</code></pre>
</li>
<li><p>I'd say a nested list-of-lists is a good choice here, and your algorithm (even before 2.) should already be as efficient as possible.</p>
</li>
</ol>
<hr />
<p>Having said that, I noticed you have a <code>comb</code> function, so maybe your goal is not really to print the triangle, but to calculate individual values. In this case, there are two possible ways to make you code faster (although I did not actually time it).</p>
<p>First, you could use a <code>dict</code> as data structure and then only calculate the values that are actually needed to find the value <code>at</code> a given <code>row</code> and <code>col</code>. In the worst case (centre of bottom row) that will be 50% of the entire triangle, and on average much less than that.</p>
<pre><code>class Pascal_Triangle:
    def __init__(self):
        self.data = {(0, 0): 1}
        
    def fill_rows(self, rows: int):
        # actually, just the last row would be enough here...
        for row in range(rows + 1):
            for col in range(row + 1):
                self.at(row, col)
        
    def at(self, row: int, col: int) -&gt; int:
        if not 0 &lt;= col &lt;= row:
            raise ValueError(f'column position {col} is invalid for row {row}')
        if (row, col) not in self.data:
            self.data[row, col] = 1 if col in (0, row) else self.at(row - 1, col - 1) + self.at(row - 1, col)
        return self.data[row, col]
    
    def pretty_print(self):
        longest = max(len(str(n)) for n in self.data.values())
        max_row = max(row for (row, col) in self.data)
        line_length = (longest + 1) * max_row
        for row in range(max_row+1):
            print(' '.join(str(self.data.get((row,col), &quot;&quot;)).center(longest) for col in range(row + 1)).center(line_length))
</code></pre>
<p>This version still has the <code>fill_rows</code> and <code>pretty_print</code> functions (nicely showing which values were actually calculated). If you don't need those, you could also just make <code>at</code> a function and use <code>functools.cache</code> to cache the values...</p>
<pre><code>from functools import cache

@cache
def at(row: int, col: int) -&gt; int:
    if not 0 &lt;= col &lt;= row:
        raise ValueError(f'column position {col} is invalid for row {row}')
    return 1 if col in (0, row) else at(row - 1, col - 1) + at(row - 1, col)
</code></pre>
<p>... or calculate the binomial coefficient directly using factorials:</p>
<pre><code>    from math import factorial as fac
    def comb(n, k):
        return fac(n) // (fac(k)*(fac(n-k)))
</code></pre>


Title: How to accelerate the convergence of Leibniz Series?
Tags: <python><python-3.x><algorithm><performance><pi>
Body: <p>The <a href="https://en.wikipedia.org/wiki/Leibniz_formula_for_%CF%80" rel="nofollow noreferrer">Leibniz series</a> is the following infinite series that is used to approximate π. It does indeed converge to π but it does so in a notoriously slow way.</p>
<p><img src="https://wikimedia.org/api/rest_v1/media/math/render/svg/b702b40ec6c3f81e02a697312d2939a1068b467d" alt="" /></p>
<p>A few years ago I wrote many Python functions to calculate π, and of course I wrote a function using it (because it is in many online tutorials), but finding it inefficient, I wrote much more efficient functions that approximate π much faster, the fastest I have tested was <a href="https://en.wikipedia.org/wiki/Ramanujan%E2%80%93Sato_series" rel="nofollow noreferrer">Ramanujan's π formula</a>, it gives 15 correct decimal places in 3 iterations (at this point double underflows).</p>
<p>A while back I watched this <a href="https://www.youtube.com/watch?v=ypxKzWi-Bwg" rel="nofollow noreferrer">video</a> about accelerating Leibniz Series by using correction terms, and I was intrigued, so I wrote some Python functions for the correction terms to check the video's veracity, I found that using 1 correction term the series yields 10 correct decimal places for 2048 iterations, 2 correction terms yields 15 correct decimal places for 638 iterations, 3 correction terms yields 15 correct decimal places in 131 iterations, but 4 correction terms makes the series converge slower, makes the underflow occur at 154th iteration.</p>
<p>I read that Leibniz Series can be accelerated:</p>
<blockquote>
<p>However, the Leibniz formula can be used to calculate π to high precision (hundreds of digits or more) using various <a href="https://en.wikipedia.org/wiki/Convergence_acceleration" rel="nofollow noreferrer">convergence acceleration</a> techniques.</p>
</blockquote>
<p>So I wanted to implement the algorithms myself, as a self-imposed programming challenge, to improve my skills (if anyone's wondering, no this is not homework), but my math isn't that good, so I put off doing it, until very recently when I read <a href="https://www.codeproject.com/Articles/5353031/Summation-of-Series-with-Convergence-Acceleration" rel="nofollow noreferrer">this</a>.</p>
<p>I tried to implement some algorithms but I found that while they need less terms for more precise result, the computation for the same number of terms is much more expensive, so that they end up taking longer time to execute. Below is my code, I want to know, what programming techniques and convergence acceleration methods (limited to ones found on the Leibniz series Wikipedia page) I can use, to speed up the calculation of π using the Leibniz Series, both making it use less number of terms, and take less time to execute?</p>
<p>(Using compiled libraries is OK, but I don't want to cheat, by caching for example)</p>
<pre class="lang-py prettyprint-override"><code>import math
from itertools import cycle
from math import factorial


def Ramanujan(n):
    def term(k): return factorial(4 * k) * (1103 + 26390 * k) / \
        (factorial(k) ** 4 * 396 ** (4 * k))
    s = sum(term(i) for i in range(n))
    return 1 / (2 * 2 ** 0.5 * s / 9801)


sign = [1, -1]


def Leibniz_Series(n):
    return [1/i*s for i, s in zip(range(1, 2*n+1, 2), cycle(sign))]


def Leibniz_0(n):
    return 4 * sum(Leibniz_Series(n))


def Leibniz_1(n):
    correction = 1 / n*sign[n % 2]
    return Leibniz_0(n) + correction


def Leibniz_2(n):
    correction = 1 / (
        n + 1 / (
            4 * n
        )
    )*sign[n % 2]
    return Leibniz_0(n) + correction


def Leibniz_3(n):
    correction = 1 / (
        n + 1 / (
            4 * n + 4 / n
        )
    )*sign[n % 2]
    return Leibniz_0(n) + correction


def Leibniz_4(n):
    correction = 1 / (
        n + 1 / (
            4 * n + 4 / (
                n + 9 / n
            )
        )
    )*sign[n % 2]
    return Leibniz_0(n) + correction


def bincoeff(n, k):
    r = 1
    if (k &gt; n):
        return 0
    for d in range(1, k + 1):
        r = r * n / d
        n -= 1
    return r


def abs(n):
    return n * (-1) ** (n &lt; 0)


def Euler(arr):
    out = []

    for i in range(len(arr)):
        delta = 0
        for j in range(i + 1):
            coeff = bincoeff(i, j)
            delta += (-1) ** j * coeff * abs(arr[j])

        out.append(0.5 ** (i + 1) * delta)
    return out


def Leibniz_Euler(n):
    return 4 * sum(Euler(Leibniz_Series(n)))


def Leibniz_Aitken(n):
    series = Leibniz_Series(n)
    s0, s1, s2 = (sum(series[:n-i]) for i in (2, 1, 0))
    return 4 * (s2 * s0 - s1 * s1) / (s2 - 2*s1 + s0)


def LCP(s1, s2):
    i = 0
    for a, b in zip(s1, s2):
        if a != b:
            break
        i += 1
    return i


for func in (Ramanujan, Leibniz_Euler, Leibniz_Aitken, Leibniz_4, Leibniz_3, Leibniz_2, Leibniz_1, Leibniz_0):
    i = 12
    last = 0
    while (pi := func(i)) != math.pi:
        i += 1
        if i == 2048:
            if abs(pi - math.pi) &lt;= 1e-6:
                print('the calculated result is close')
            else:
                print('the method is too slow')
            break
        if pi == last:
            print('float underflow')
            break
        last = pi
    print(
        f'function: {func.__name__}, iterations: {i}, correctness: {LCP(str(pi), str(math.pi))}')
</code></pre>
<p>Performance:</p>
<pre><code>function: Ramanujan, iterations: 12, correctness: 17
float underflow
function: Leibniz_Euler, iterations: 52, correctness: 16
the calculated result is close
function: Leibniz_Aitken, iterations: 2048, correctness: 11
function: Leibniz_4, iterations: 154, correctness: 17
function: Leibniz_3, iterations: 131, correctness: 17
function: Leibniz_2, iterations: 638, correctness: 17
the calculated result is close
function: Leibniz_1, iterations: 2048, correctness: 12
the method is too slow
function: Leibniz_0, iterations: 2048, correctness: 4

In [2]: %timeit Ramanujan(3)
3.7 µs ± 229 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)

In [3]: %timeit Leibniz_3(131)
17.4 µs ± 123 ns per loop (mean ± std. dev. of 7 runs, 100,000 loops each)

In [4]: %timeit Leibniz_Aitken(2048)
295 µs ± 10.8 µs per loop (mean ± std. dev. of 7 runs, 1,000 loops each)

In [5]: %timeit Leibniz_Euler(52)
3.81 ms ± 430 µs per loop (mean ± std. dev. of 7 runs, 100 loops each)
</code></pre>


Answer: <h2>Reducing the number of terms</h2>
<p>You can <strong>compress the series by computing terms 2 by 2</strong>. Indeed, <code>1/a - 1/(a+2) = 2/(a(a+2))</code>. Since, 2 is a constant here, you can compute the sum of the terms : <code>2 * (1/(1*3) + 1/(5*7) + 1/(9*11) + ...)</code>. Note that each term is a bit more expensive to compute but the slowest operation is actually the division and there are twice less divisions to compute for a similar target precision. If we try to apply the same method to compress the series further (by packing terms 4 by 4), then the terms starts to be significantly more complex to compute so the benefit is not so big. Indeed, for each group of <strong>4 terms</strong>, we can compute <code>b, c, d = a+2, a+4, a+6</code> and then compute compressed terms using <code>2 (a b + c d) / (a b c d)</code>. Note the constant 2 can still be factorized when computing multiple terms. With this last method, there are 5 mul + 1 add + 1 div per term while the previous method required 2 mul + 1 add + 2 div per pair of terms (similar precision). Thus, it does not worth it unless the cost of the division is huge compared multiplication (i.e. <code>cost(1 div) &gt; cost(3 mul)</code>). This is actually the case on mainstream x86-64 processor (e.g. 4 cycle/div versus 0.5 cycle/mul for scalar native computations on relatively-recent Intel processors). Compressing the series further is just not worth it on all mainstream architecture in practice, not to mention it makes the terms far more complex.</p>
<p>The same logic is true for <code>Leibniz_Euler</code> and <code>Leibniz_Aitken</code>. You can strongly reduce the number of terms to compute easily but generally at the expense of more expensive terms to compute (unless the target series is trivial). This methods can be generalized to any algorithm or even processor and programming languages. More specifically, it can be applies to Turing machines (i.e. acceleration theorem). Again, in this case, it requires an exponential number of states and/or symbols to compress a given Turing machine (unless it can be simplified, so unless it is a trivial one). There is no free lunch. </p>
<p>However, note that <code>Euler</code> has a sub-optimal <strong>time complexity</strong> : it runs in <code>O(n**3)</code> time while it can be computed in <code>O(n**2)</code> time. Indeed, the <code>bincoeff</code> values can be precomputed using the <a href="https://en.wikipedia.org/wiki/Pascal%27s_triangle" rel="nofollow noreferrer">Pascal's triangle</a>. It should significantly speed up the computation so this method can be about as fast as (or a bit faster then) the <code>Leibniz_Aitken</code> one.</p>
<hr />
<h2>Computing terms much faster</h2>
<p>CPython is an interpreter and <strong>interpreters are very slow</strong> for such a computation. You should avoid CPython if you want things to be computed quickly. There are many solutions to do that. The most famous one is using <strong>vectorization</strong>, that is calling native (C/C++) functions to compute most of the work without the interpreter. Numpy is a good example for that. Another method is to use compilers so to generate native functions from a Python codes. <strong>Numba and Cython</strong> are example of modules doing this very well. The later is generally more efficient than the former (especially if you are a skilled developer).</p>
<p>Based on the compressed series described above, we can write a very-fast compiled implementation (compared to the reference implementation):</p>
<pre class="lang-py prettyprint-override"><code>@nb.njit('float64(int64)')
def Leibniz_fast(n):
    iMax = 2*n+1

    # Compressed terms
    s1 = 0.0
    iCompMax = (2*n+1-8)//8*8+1
    i = iCompMax
    while i &gt;= 1:
        a = float(i)
        b = a + 2
        c = a + 4
        d = a + 6
        s1 += (a*b + c*d) / ((a*b)*(c*d))
        i -= 8

    # Remaining non-compressed terms
    s2 = 0.0
    if iCompMax +  8 &lt; iMax:  s2 += 1.0 / (iCompMax +  8)
    if iCompMax + 10 &lt; iMax:  s2 -= 1.0 / (iCompMax + 10)
    if iCompMax + 12 &lt; iMax:  s2 += 1.0 / (iCompMax + 12)

    # Correction
    sign = 1 if n % 2 == 0 else -1
    correction = sign / (n + 1 / (4 * n + 4 / n))   # From Leibniz_3

    return 8 * s1 + 4 * s2 + correction
</code></pre>
<hr />
<h2>Performance benchmark</h2>
<p>Here are performance results on my i5-9600KF processor with CPython 3.8.1.</p>
<pre class="lang-none prettyprint-override"><code>Leibniz_3(131):      12.6 µs
Leibniz_fast(131):    0.2 µs

Leibniz_3(500):      47.6 µs
Leibniz_fast(500):    0.3 µs
</code></pre>
<p><code>Leibniz_fast</code> is about ~100 times faster. Most of the speed-up is coming from the compilation of the code.</p>
<hr />
<h2>Note about the accuracy of the solutions</h2>
<p>The precision of <code>Leibniz_fast</code> is close to <code>Leibniz_3</code>. Both achieve an accuracy of few <a href="https://en.wikipedia.org/wiki/Unit_in_the_last_place" rel="nofollow noreferrer">ULP</a> for small <code>n</code> values. For large <code>n</code> values, both are quite inaccurate because of the sum accumulating errors. Note <code>Leibniz_fast</code> compute compressed terms in a reverse order so to accumulate small values together and try to add values of the same order. That being said, this is not enough to get a very accurate solution for large <code>n</code> values. One need to perform a more accurate sum for example using chunks or even using a <a href="https://en.wikipedia.org/wiki/Kahan_summation_algorithm" rel="nofollow noreferrer">Kahan summation algorithm</a> at the expense of a slower computation. An alternative solution is to use libraries like GMP (faster than the default <code>decimal</code> package) so to compute numbers with an arbitrary precision though it is much slower than operating on double-precision numbers.</p>
<p>The number of terms required for <code>Leibniz_fast</code> to reach the same accuracy than <code>Leibniz_3</code> is 4 times smaller (less than 40 terms are needed for double-precision numbers). Indeed, terms are computed 4 by 4 with a step of 8 in the <code>while</code> loop.</p>
<p>If you want to compute precisely pi, then the Ramanujan–Sato series (mentioned in the question) is a very good one when combined with libraries like GMP. The result is both very-fast and precise. Note the factorial terms can be precomputed so to improve the complexity compared to your naive implementation. Besides some factorial terms can also be skipped so to make the number smaller in memory (this is critical since the numbers grows exponentially and the amount of RAM used can quickly be an issue).</p>


